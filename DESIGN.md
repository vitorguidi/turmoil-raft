# Turmoil Raft Implementation Design

This document is a low-level specification for implementing Raft using Rust, Tonic, and Turmoil. It is divided into Phases corresponding to the MIT 6.824 labs.

## Phase 0: Networking & Boilerplate (CRITICAL FIRST STEP)

Before any Raft logic, we must make Tonic (gRPC) work inside Turmoil's simulation. Turmoil simulates the network; Tonic usually expects OS sockets. We bridge them.

### `src/utils/mod.rs`
**Goal:** Create a "Connector" that allows Tonic clients to dial Turmoil nodes.

**Implementation Details:**
- Define `pub async fn grpc_connector(addr: SocketAddr) -> Result<impl ...>`
- Use `turmoil::net::TcpStream::connect(addr)`.
- **Why?** We will pass this connector to `tonic::transport::Endpoint::connect_with_connector`.

---

## Phase 1: Lab 2A - Leader Election

**Goal:** Nodes elect a leader, maintain terms, and send empty Heartbeats.

### `src/raft/core.rs` (The Brain)
**Struct `Raft`:**
- **State Fields:**
  - `role`: Enum `Follower | Candidate | Leader`
  - `current_term`: `u64` (Persistent)
  - `voted_for`: `Option<u64>` (Persistent)
  - `log`: `Vec<LogEntry>` (Persistent) - *Initially empty*
  - `commit_index`: `u64`
  - `last_applied`: `u64`
- **Channels:**
  - `rpc_rx`: Receives `RaftRpc` enum (defined below) from the gRPC server.
  - `apply_tx`: Sends committed logs to the State Machine (KV Store).

**Enum `RaftRpc`:**
- Internal enum to pass incoming RPCs from `src/raft/rpc.rs` into the main `tokio::select!` loop.
- Variants: `RequestVote(Request, ReplySender)`, `AppendEntries(Request, ReplySender)`.

**Main Loop (`Raft::run`)**:
- Use `tokio::select!`.
- **Branch 1: Election Timer**:
  - If timeout && role != Leader: Become Candidate, increment term, vote self, send RequestVote to all peers.
- **Branch 2: Heartbeat Timer**:
  - If Leader: Send AppendEntries (empty) to all peers.
- **Branch 3: `rpc_rx.recv()`**:
  - Handle `RequestVote`: Check term, grant vote if not voted & log is up-to-date.
  - Handle `AppendEntries`: Check term, reset election timer.

### `src/raft/rpc.rs` (The Mouth)
**Struct `RaftService`:**
- Implements `raft::raft_server::Raft` trait (generated by Tonic).
- Holds a `tokio::sync::mpsc::Sender<RaftRpc>`.
- **Logic:**
  - When `request_vote(...)` is called by Tonic:
    1. Create a `oneshot::channel` for the reply.
    2. Wrap request + oneshot sender into `RaftRpc::RequestVote`.
    3. Send to `core.rs` via mpsc.
    4. Await oneshot receiver and return response.

### `src/raft/client.rs` (The Hands)
**Struct `RaftClient`:**
- Wrapper around `raft::raft_client::RaftClient<Channel>`.
- **Method `send_request_vote`:**
  - Connects to peer using `src/utils/connector`.
  - Sets a timeout (using `tower::timeout` or `tokio::time::timeout`).
  - Returns `Result<RequestVoteResponse>`.

### `tests/raft_election.rs`
- **Test `basic_election`:**
  - `turmoil::Builder::new()`.
  - `sim.host("node-1", ...)`: Spawns the Raft node.
  - Run for 1-2 seconds.
  - Assert strictly ONE leader exists (check internal state or logs).

---

## Phase 2: Lab 2B - Log Replication

**Goal:** The Leader accepts commands, replicates to followers, and commits.

### `src/raft/core.rs`
- **Update `Raft` Struct:**
  - Add `next_index`: `Vec<u64>` (Leader only).
  - Add `match_index`: `Vec<u64>` (Leader only).
- **Update Loop:**
  - **Start Command:** Add public method `start(command: Bytes)`. It appends to local log and wakes up the "replicator" (or sends immediate heartbeats).
  - **Handle AppendEntries (Follower):**
    - **Consistency Check:** If `prev_log_index` term mismatch -> Return false.
    - **Conflict Resolution:** Truncate log if mismatch.
    - **Append:** Add new entries.
    - **Commit:** Update `commit_index = min(leader_commit, last_new_entry_index)`.
  - **Handle AppendEntries Response (Leader):**
    - If success: Update `match_index`, `next_index`. Check if majority > `commit_index`, then update `commit_index` and notify apply channel.
    - If fail: Decrement `next_index` and retry (simple backtracking) or use "fast backup" optimization (Lab 2C prep).

### `src/raft/log.rs`
- Helper methods to handle the 1-based indexing confusion.
- `get(i)`: returns `log[i-1]`.
- `slice(start, end)`: safely returns a range of entries.

---

## Phase 3: Lab 2C - Persistence

**Goal:** Survive `sim.crash()` and `sim.bounce()`.

### `src/raft/core.rs`
- **Dependency:** Add `serde` and `serde_json` (or `bincode`).
- **Logic:**
  - Define `PersistState` struct (term, vote, log).
  - In `turmoil`, genericize `Raft` with a "Store" trait, or just use a simple file path `std::fs::write` (Turmoil mocks filesystem!).
  - **Write:** Call `persist()` whenever `current_term`, `voted_for`, or `log` changes.
  - **Read:** On startup (`Raft::new`), read file and populate fields.

### `tests/raft_persistence.rs`
- **Test `crash_leader`:**
  1. Elect Leader.
  2. Commit log entry.
  3. `sim.crash(leader)`.
  4. `sim.bounce(leader)`.
  5. Leader should remember the log entry and term.

---

## Phase 4: Lab 3 - KV Server

**Goal:** The actual application.

### `proto/kv.proto`
- Ensure `Get`, `Put`, `Append` messages are correct.

### `src/kv/server.rs`
**Struct `KvServer`:**
- Implements `kv::kv_server::Kv`.
- Holds `Arc<Mutex<Raft>>` (or a channel to it).
- **State:** `HashMap<String, String>` (The Store).
- **Duplicate Table:** `HashMap<u64, RequestResult>` (Key: ClientID).
- **Logic:**
  - `Put(key, value)`:
    1. Serialize op.
    2. `raft.start(op)`.
    3. Watch `apply_ch` for the result.
    4. **Timeout:** If Raft doesn't commit in X ms, return "Wrong Leader" or "Timeout".

### `src/kv/client.rs` (Clerk)
**Struct `Clerk`:**
- Keeps track of `leader_id`.
- **Retry Loop:**
  - Send RPC to `leader_id`.
  - If success: return.
  - If fail/timeout/wrong_leader: `leader_id = (leader_id + 1) % n_servers`, sleep, retry.

---

## Phase 5: Lab 4 - Sharding (Advanced)

*Detailed design for this phase will be generated after Lab 3 is complete.*

## Implementation Strategy & Hints

1.  **Debugging:** Use `tracing::info!(node = ?self.id, term = self.term, "Event description");`.
2.  **Concurrency:** Do **not** use `Mutex` inside `Raft` core methods if you can avoid it. The `tokio::select!` loop effectively serializes access to the state.
3.  **Deadlocks:** Watch out for waiting on a channel in the main loop that depends on the main loop processing a message.
4.  **Turmoil:**
    - `sim.client("client", async move { ... })` simulates an external client.
    - `sim.run()` runs until the future completes or `max_duration` is hit.